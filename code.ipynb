{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "30ef9a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cb119f",
   "metadata": {},
   "source": [
    "## Input Embeddings and Positional encoding - Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "03a40148",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "e1bb9026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<pad>': 0, '<unk>': 1, '<sos>': 2, '<eos>': 3, 'we': 4, 'are': 5, 'friends': 6, 'हम': 7, 'दोस्त': 8, 'हैं': 9}\n"
     ]
    }
   ],
   "source": [
    "SPECIAL_TOKENS = {\n",
    "    '<pad>': 0,\n",
    "    '<unk>': 1,\n",
    "    '<sos>': 2,\n",
    "    '<eos>': 3,\n",
    "}\n",
    "source_sentences = [\"We are friends\"]\n",
    "target_sentences = [\"हम दोस्त हैं\"]\n",
    "sentences = source_sentences + target_sentences\n",
    "\n",
    "def build_vocab(sentences):\n",
    "    vocab = dict(SPECIAL_TOKENS)\n",
    "    idx = len(vocab)\n",
    "    \n",
    "    for s in sentences:\n",
    "        tokens = s.lower().split()\n",
    "        for token in tokens:\n",
    "            if token not in vocab:\n",
    "                vocab[token] = idx\n",
    "                idx +=1\n",
    "    return vocab\n",
    "\n",
    "vocab = build_vocab(sentences)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "801b7fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4, 5, 6]]) \n",
      " torch.Size([1, 3])\n"
     ]
    }
   ],
   "source": [
    "input = \"We are friends\"\n",
    "\n",
    "def tokenize(sentence, vocab):\n",
    "    tokens = sentence.lower().split()\n",
    "    ids = [vocab.get(t, vocab[\"<unk>\"]) for t in tokens]\n",
    "    input_ids = torch.tensor(ids).unsqueeze(0) # (3,) -> (1,3)\n",
    "    return input_ids\n",
    "    \n",
    "input_ids = tokenize(input, vocab)\n",
    "print(input_ids,\"\\n\",input_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "0064958f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[  5.9759, -42.0194, -15.8839,  ...,  28.2713,  38.8521, -24.3929],\n",
      "         [ 18.0416, -14.5304, -35.0097,  ...,   8.5690,   1.9877,   7.9900],\n",
      "         [  4.7285,  -9.1166, -12.1504,  ..., -25.0507, -21.2639,  24.7698]]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "torch.Size([1, 3, 512])\n"
     ]
    }
   ],
   "source": [
    "embedding = nn.Embedding(num_embeddings=len(vocab), embedding_dim=d_model)\n",
    "input_embeddings = embedding(input_ids) * math.sqrt(d_model)\n",
    "print(input_embeddings)\n",
    "print(input_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "5c3607ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  ...,  1.0000e+00,\n",
      "           0.0000e+00,  1.0000e+00],\n",
      "         [ 8.4147e-01,  5.4030e-01,  8.0196e-01,  ...,  1.0000e+00,\n",
      "           1.0746e-08,  1.0000e+00],\n",
      "         [ 9.0930e-01, -4.1615e-01,  9.5814e-01,  ...,  1.0000e+00,\n",
      "           2.1492e-08,  1.0000e+00]]])\n",
      "torch.Size([1, 3, 512])\n"
     ]
    }
   ],
   "source": [
    "def positionalEncoding(input_embeddings, d_model=512):\n",
    "    seq_length = input_embeddings.shape[1] if input_embeddings.dim() == 3 else input_embeddings.shape[0]   # 3\n",
    "    positional_encoding = torch.zeros(seq_length, d_model, device=input_embeddings.device)\n",
    "    \n",
    "    for pos in range(seq_length):\n",
    "        for i in range(0, d_model, 2):  # [sin, cos, sin, cos , ...,sin, cos] for 512 dimensions\n",
    "            PE_sin = math.sin(pos / 10000**(2*i/d_model))\n",
    "            PE_cos = math.cos(pos / 10000**(2*i/d_model))\n",
    "            positional_encoding[pos, i] = PE_sin\n",
    "            positional_encoding[pos, i+1] = PE_cos\n",
    "\n",
    "    return positional_encoding.unsqueeze(0)\n",
    "\n",
    "positional_encoding = positionalEncoding(input_embeddings)\n",
    "print(positional_encoding)\n",
    "print(positional_encoding.shape)   # (3, 512)\n",
    "\n",
    "\n",
    "def positionalEncoding2(input_embeddings, d_model=512):\n",
    "    \"\"\"\n",
    "    x: (B, seq_len, d_model)\n",
    "    Returns: (1, seq_len, d_model) on same device as x\n",
    "    \"\"\"\n",
    "    seq_len = input_embeddings.shape[1]\n",
    "    device = input_embeddings.device\n",
    "    position = torch.arange(0, seq_len, dtype=torch.float, device=device).unsqueeze(1)\n",
    "    div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float, device=device) * \n",
    "                         (-math.log(10000.0) / d_model))\n",
    "    \n",
    "    pe = torch.zeros(seq_len, d_model, device=device)\n",
    "    pe[:, 0::2] = torch.sin(position * div_term)\n",
    "    pe[:, 1::2] = torch.cos(position * div_term)\n",
    "    \n",
    "    return pe.unsqueeze(0)  # (1, seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "232fc9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_padding_mask(input_ids, pad_idx=0):\n",
    "    # (b, l) -> (b, 1, 1, l)\n",
    "    return (input_ids != pad_idx).unsqueeze(1).unsqueeze(1)\n",
    "\n",
    "def make_causal_mask(tgt_len, device):  # (1, tgt_len, tgt_len)\n",
    "    return torch.tril(torch.ones((tgt_len, tgt_len), dtype=torch.bool, device=device)).unsqueeze(0) \n",
    "\n",
    "def combine_padding_and_causal(tgt_pad_mask, causal_mask):    # tgt_pad_mask: (B, 1, 1, L) -> convert to (B, L) valid positions\n",
    "    batch_size = tgt_pad_mask.shape[0]\n",
    "    L = tgt_pad_mask.shape[-1]\n",
    "    valid = tgt_pad_mask.squeeze(1).squeeze(1)   # (batch_size, L)\n",
    "    causal_b = causal_mask.expand(batch_size, -1, -1)   # (batch_size, L, L)\n",
    "    valid_src = valid.unsqueeze(1).expand(-1, L, -1)   # (batch_size, L, L)\n",
    "    combined = causal_b & valid_src\n",
    "    return combined.unsqueeze(1)  # (batch, 1, L, L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "42bca895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[  5.9759, -41.0194, -15.8839,  ...,  29.2713,  38.8521, -23.3929],\n",
      "         [ 18.8831, -13.9901, -34.2077,  ...,   9.5690,   1.9877,   8.9900],\n",
      "         [  5.6378,  -9.5328, -11.1922,  ..., -24.0507, -21.2639,  25.7698]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "torch.Size([1, 3, 512])\n"
     ]
    }
   ],
   "source": [
    "x = input_embeddings + positional_encoding\n",
    "print(x)\n",
    "print(x.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad676316",
   "metadata": {},
   "source": [
    "## Encoder Sub-layer 1 = Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "7cc084da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ -5.7154,  -3.6326,  -2.1424,  ...,   0.6965,   4.7687,  11.3503],\n",
      "         [  6.5223,  -0.0000, -11.3013,  ...,  -5.8819,  -1.1839,  -2.2014],\n",
      "         [ -3.8892,  -3.0986,  -5.4991,  ...,   7.6042,   7.8150,  11.6261]]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "torch.Size([1, 3, 512])\n"
     ]
    }
   ],
   "source": [
    "# class ScaledDotProductAttention(nn.Module):\n",
    "#     def __init__(self, d_k):\n",
    "#         super().__init__()\n",
    "# W_Q = torch.randn(x.shape[2], x.shape[2])   # (512, 512)\n",
    "# W_K = torch.randn(x.shape[2], x.shape[2])\n",
    "# W_V = torch.randn(x.shape[2], x.shape[2])\n",
    "\n",
    "# Q = x @ W_Q  # (1, 3, 512)\n",
    "# K = x @ W_K\n",
    "# V = x @ W_V\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, d_k):\n",
    "        super().__init__()\n",
    "        self.d_k = d_k\n",
    "    \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        attention = torch.matmul(Q, K.transpose(-2,-1)) / math.sqrt(self.d_k)\n",
    "        if (mask is not None):\n",
    "            attention = attention.masked_fill(~mask, float('-inf'))\n",
    "        \n",
    "        attention_weights = torch.softmax(attention, dim=-1)\n",
    "        output = torch.matmul(attention_weights, V)\n",
    "        return output, attention_weights\n",
    "    \n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,d_model=512, heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.heads = heads\n",
    "        self.d_k = d_model // heads    # 512//8 = 64 because they are concatenated later\n",
    "        \n",
    "        self.W_Q = nn.Linear(d_model, d_model)\n",
    "        self.W_K = nn.Linear(d_model, d_model)\n",
    "        self.W_V = nn.Linear(d_model, d_model)\n",
    "        self.attention = ScaledDotProductAttention(self.d_k)\n",
    "        self.z = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        # Linear\n",
    "        Q = self.W_Q(q)   # (1, 3, 512)\n",
    "        K = self.W_K(k)\n",
    "        V = self.W_V(v)\n",
    "        \n",
    "        batch_size, tgt_len, _ = Q.shape\n",
    "        _, src_len, _ = K.shape\n",
    "        # heads\n",
    "        Q = Q.view(batch_size, tgt_len, self.heads, self.d_k)   # (1, 3, 8, 64)\n",
    "        K = K.view(batch_size, src_len, self.heads, self.d_k)\n",
    "        V = V.view(batch_size, src_len, self.heads, self.d_k)\n",
    "        \n",
    "        Q = Q.transpose(1,2)    # (1, 8, 3, 64)\n",
    "        K = K.transpose(1,2)\n",
    "        V = V.transpose(1,2)\n",
    "        \n",
    "        # attention\n",
    "        if mask is not None:\n",
    "            if mask.dim() == 2:\n",
    "                mask = mask.unsqueeze(0).unsqueeze(0)    # (1, 1, tgt_len, src_len) = (1, 1, 3, 3)\n",
    "            elif mask.dim() == 3:\n",
    "                mask = mask.unsqueeze(1)     # (1, 1, 3, 3)\n",
    "            elif mask.dim() == 4:\n",
    "                pass\n",
    "            mask = mask.to(torch.bool)\n",
    "                \n",
    "                \n",
    "        output, attention_weights = self.attention(Q, K, V, mask=mask)    # (1, 8, 3, 64) ; (1, 8, 3, 3)\n",
    "        # concat\n",
    "        concat_output = output.transpose(1,2)     # (1, 3, 8, 64)\n",
    "        concat_output = concat_output.contiguous().view(batch_size, tgt_len, self.d_model)   # (1, 3, 512)\n",
    "        # linear\n",
    "        output = self.z(concat_output)\n",
    "        \n",
    "        output = self.dropout(output)\n",
    "        \n",
    "        return output\n",
    "        \n",
    "heads = 8\n",
    "mha = MultiHeadAttention(d_model, heads)\n",
    "mha_output = mha(x, x, x)   # (1, 3, 512)\n",
    "print(mha_output)\n",
    "print(mha_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ff2dde",
   "metadata": {},
   "source": [
    "## Feed Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "f6666d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, inner_dim=4*d_model, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.inner_dim = inner_dim\n",
    "        self.l1 = nn.Linear(d_model, inner_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.l2 = nn.Linear(inner_dim, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.dropout(self.l2(self.relu(self.l1(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9549983",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "a1c9bd79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.1140, -1.6504, -1.0796,  ...,  0.8825, -0.0107, -0.7957],\n",
      "         [ 0.5929, -1.0266, -2.0543,  ...,  0.0375, -0.2036, -0.0611],\n",
      "         [ 0.0303, -0.9900, -0.9896,  ..., -0.7503,  0.0853,  0.7605]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "torch.Size([1, 3, 512])\n"
     ]
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, d_model, heads, inner_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.heads = heads\n",
    "        self.inner_dim = inner_dim\n",
    "\n",
    "        self.attention = MultiHeadAttention(d_model=self.d_model, heads=self.heads)\n",
    "        self.layer_norm1 = nn.LayerNorm(self.d_model)\n",
    "        self.layer_norm2 = nn.LayerNorm(self.d_model)\n",
    "        self.ffn = FeedForward(self.d_model, inner_dim=4*self.d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, src_mask=None):\n",
    "        y1 = self.attention(x, x, x, mask = src_mask)\n",
    "        x = x + y1\n",
    "        x = self.layer_norm1(x)\n",
    "        \n",
    "        y2 = self.ffn(x)\n",
    "        x = x + y2\n",
    "        encoder_output = self.layer_norm2(x)\n",
    "        \n",
    "        return encoder_output\n",
    "\n",
    "input = \"We are friends\"\n",
    "input_ids = tokenize(input, vocab)\n",
    "input_embeddings = embedding(input_ids) * math.sqrt(d_model)\n",
    "positional_encoding = positionalEncoding(input_embeddings, d_model)\n",
    "x = input_embeddings + positional_encoding\n",
    "dropout = nn.Dropout(p=0.1)\n",
    "x = dropout(x)\n",
    "\n",
    "encoder = Encoder(d_model=512, heads=8, inner_dim=2048)\n",
    "encoder_output = encoder(x)   # (1, 3, 512)\n",
    "print(encoder_output)\n",
    "print(encoder_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637c3afd",
   "metadata": {},
   "source": [
    "## Decoder block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "0d9fcaef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2, 7, 8]])\n",
      "tensor([[7, 8, 9]])\n"
     ]
    }
   ],
   "source": [
    "target_sentence = \"हम दोस्त हैं\"\n",
    "target_ids = tokenize(target_sentence, vocab)\n",
    "\n",
    "def shift_right(target_ids):\n",
    "    batch_size = target_ids.shape[0]\n",
    "    sos = vocab['<sos>']\n",
    "    sos_tensor = torch.full((batch_size, 1), sos, device=target_ids.device, dtype=torch.long)\n",
    "    return torch.cat([sos_tensor, target_ids[:, :-1]], dim=1)\n",
    "\n",
    "decoder_input = shift_right(target_ids)\n",
    "decoder_labels = target_ids\n",
    "\n",
    "print(decoder_input)   # <sos> हम दोस्त\n",
    "print(decoder_labels)  # हम दोस्त हैं"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "614e51e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.2399, -0.0883, -1.1978,  ...,  0.7475, -0.8898,  0.0947],\n",
      "         [ 0.1491, -1.6535, -0.3827,  ...,  1.7874,  0.3774,  0.7398],\n",
      "         [-1.0974, -2.1587,  2.0471,  ...,  1.8877,  0.7828, -1.1030]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "torch.Size([1, 3, 512])\n"
     ]
    }
   ],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, d_model, heads, inner_dim=4*d_model, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.heads = heads\n",
    "        self.inner_dim = inner_dim\n",
    "        \n",
    "        self.masked_attention = MultiHeadAttention(d_model=self.d_model, heads=self.heads)\n",
    "        self.layer_norm1 = nn.LayerNorm(self.d_model)\n",
    "        self.cross_attention = MultiHeadAttention(self.d_model, self.heads)\n",
    "        self.layer_norm2 = nn.LayerNorm(self.d_model)\n",
    "        self.ffn = FeedForward(d_model=self.d_model)\n",
    "        self.layer_norm3 = nn.LayerNorm(self.d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, encoder_output, tgt_mask=None, src_mask=None):\n",
    "        y1 = self.masked_attention(x, x, x, mask=tgt_mask)\n",
    "        x = x + y1\n",
    "        x = self.layer_norm1(x)\n",
    "        \n",
    "        y2 = self.cross_attention(x, encoder_output, encoder_output, mask=src_mask)\n",
    "        x = x + y2\n",
    "        x = self.layer_norm2(x)\n",
    "        \n",
    "        y3 = self.ffn(x)\n",
    "        x = x + y3\n",
    "        decoder_output = self.layer_norm3(x)\n",
    "        \n",
    "        return decoder_output\n",
    "\n",
    "decoder_input_embeddings = embedding(decoder_input) * math.sqrt(d_model)\n",
    "decoder_positional_encoding = positionalEncoding(decoder_input_embeddings, d_model)\n",
    "x = decoder_input_embeddings + decoder_positional_encoding\n",
    "x = dropout(x)\n",
    "\n",
    "decoder = Decoder(d_model, heads)\n",
    "decoder_output = decoder(x, encoder_output)\n",
    "print(decoder_output)\n",
    "print(decoder_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10e5a38",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "32983656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (encoder_layers): ModuleList(\n",
      "    (0-5): 6 x Encoder(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (W_Q): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (W_K): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (W_V): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (attention): ScaledDotProductAttention()\n",
      "        (z): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (ffn): FeedForward(\n",
      "        (l1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (relu): ReLU()\n",
      "        (l2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (decoder_layers): ModuleList(\n",
      "    (0-5): 6 x Decoder(\n",
      "      (masked_attention): MultiHeadAttention(\n",
      "        (W_Q): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (W_K): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (W_V): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (attention): ScaledDotProductAttention()\n",
      "        (z): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (cross_attention): MultiHeadAttention(\n",
      "        (W_Q): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (W_K): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (W_V): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (attention): ScaledDotProductAttention()\n",
      "        (z): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (ffn): FeedForward(\n",
      "        (l1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (relu): ReLU()\n",
      "        (l2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (linear): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, d_model, heads, dropout, vocab_size, num_layers, embedding=None):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.heads = heads\n",
    "        self.dropout = dropout\n",
    "        self.vocab_size = vocab_size\n",
    "        self.inner_dim = 4*self.d_model\n",
    "        self.num_layers = num_layers\n",
    "        # self.embedding = self.embedding\n",
    "        \n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            Encoder(self.d_model, self.heads, self.inner_dim, dropout=self.dropout)\n",
    "            for _ in range(self.num_layers)\n",
    "        ])\n",
    "        self.decoder_layers = nn.ModuleList([\n",
    "            Decoder(self.d_model, self.heads, self.inner_dim, dropout=self.dropout) \n",
    "            for _ in range(self.num_layers)\n",
    "        ])      \n",
    "        self.linear = nn.Linear(self.d_model, self.vocab_size, bias=True)\n",
    "        \n",
    "        if embedding is not None:  # tying weighs of linear with embedding\n",
    "            assert tuple(embedding.weight.shape) == (self.vocab_size, self.d_model), \\\n",
    "                f\"Embedding weight shape {tuple(embedding.weight.shape)} doesn't match (vocab_size, d_model)\"\n",
    "            \n",
    "            self.linear.weight = embedding.weight\n",
    "            if self.linear.bias is not None:\n",
    "                nn.init.zeros_(self.linear.bias)\n",
    "            \n",
    "    def forward(self, x_encoder, x_decoder, tgt_mask=None, src_mask=None):\n",
    "        \n",
    "        # Encoder stack\n",
    "        x = x_encoder\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x, src_mask=src_mask)\n",
    "        encoder_output = x\n",
    "        \n",
    "        # decoder stack\n",
    "        y = x_decoder\n",
    "        for layer in self.decoder_layers:\n",
    "            y = layer(y, encoder_output, tgt_mask=tgt_mask, src_mask=src_mask)\n",
    "        decoder_output = y\n",
    "        \n",
    "        logits = self.linear(decoder_output)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "d_model = 512\n",
    "heads = 8\n",
    "dropout = 0.1\n",
    "vocab_size = len(vocab)\n",
    "num_layers = 6 \n",
    "embedding=embedding\n",
    "model = Transformer(d_model, heads, dropout, vocab_size, num_layers, embedding)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "db894bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "fef67386",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==================================================================================================================================\n",
       "Layer (type:depth-idx)                                  Input Shape               Output Shape              Param #\n",
       "==================================================================================================================================\n",
       "Transformer                                             [1, 3, 512]               [1, 3, 10]                --\n",
       "├─ModuleList: 1-1                                       --                        --                        --\n",
       "│    └─Encoder: 2-1                                     [1, 3, 512]               [1, 3, 512]               --\n",
       "│    │    └─MultiHeadAttention: 3-1                     [1, 3, 512]               [1, 3, 512]               1,050,624\n",
       "│    │    └─LayerNorm: 3-2                              [1, 3, 512]               [1, 3, 512]               1,024\n",
       "│    │    └─FeedForward: 3-3                            [1, 3, 512]               [1, 3, 512]               2,099,712\n",
       "│    │    └─LayerNorm: 3-4                              [1, 3, 512]               [1, 3, 512]               1,024\n",
       "│    └─Encoder: 2-2                                     [1, 3, 512]               [1, 3, 512]               --\n",
       "│    │    └─MultiHeadAttention: 3-5                     [1, 3, 512]               [1, 3, 512]               1,050,624\n",
       "│    │    └─LayerNorm: 3-6                              [1, 3, 512]               [1, 3, 512]               1,024\n",
       "│    │    └─FeedForward: 3-7                            [1, 3, 512]               [1, 3, 512]               2,099,712\n",
       "│    │    └─LayerNorm: 3-8                              [1, 3, 512]               [1, 3, 512]               1,024\n",
       "│    └─Encoder: 2-3                                     [1, 3, 512]               [1, 3, 512]               --\n",
       "│    │    └─MultiHeadAttention: 3-9                     [1, 3, 512]               [1, 3, 512]               1,050,624\n",
       "│    │    └─LayerNorm: 3-10                             [1, 3, 512]               [1, 3, 512]               1,024\n",
       "│    │    └─FeedForward: 3-11                           [1, 3, 512]               [1, 3, 512]               2,099,712\n",
       "│    │    └─LayerNorm: 3-12                             [1, 3, 512]               [1, 3, 512]               1,024\n",
       "│    └─Encoder: 2-4                                     [1, 3, 512]               [1, 3, 512]               --\n",
       "│    │    └─MultiHeadAttention: 3-13                    [1, 3, 512]               [1, 3, 512]               1,050,624\n",
       "│    │    └─LayerNorm: 3-14                             [1, 3, 512]               [1, 3, 512]               1,024\n",
       "│    │    └─FeedForward: 3-15                           [1, 3, 512]               [1, 3, 512]               2,099,712\n",
       "│    │    └─LayerNorm: 3-16                             [1, 3, 512]               [1, 3, 512]               1,024\n",
       "│    └─Encoder: 2-5                                     [1, 3, 512]               [1, 3, 512]               --\n",
       "│    │    └─MultiHeadAttention: 3-17                    [1, 3, 512]               [1, 3, 512]               1,050,624\n",
       "│    │    └─LayerNorm: 3-18                             [1, 3, 512]               [1, 3, 512]               1,024\n",
       "│    │    └─FeedForward: 3-19                           [1, 3, 512]               [1, 3, 512]               2,099,712\n",
       "│    │    └─LayerNorm: 3-20                             [1, 3, 512]               [1, 3, 512]               1,024\n",
       "│    └─Encoder: 2-6                                     [1, 3, 512]               [1, 3, 512]               --\n",
       "│    │    └─MultiHeadAttention: 3-21                    [1, 3, 512]               [1, 3, 512]               1,050,624\n",
       "│    │    └─LayerNorm: 3-22                             [1, 3, 512]               [1, 3, 512]               1,024\n",
       "│    │    └─FeedForward: 3-23                           [1, 3, 512]               [1, 3, 512]               2,099,712\n",
       "│    │    └─LayerNorm: 3-24                             [1, 3, 512]               [1, 3, 512]               1,024\n",
       "├─ModuleList: 1-2                                       --                        --                        --\n",
       "│    └─Decoder: 2-7                                     [1, 3, 512]               [1, 3, 512]               --\n",
       "│    │    └─MultiHeadAttention: 3-25                    [1, 3, 512]               [1, 3, 512]               1,050,624\n",
       "│    │    └─LayerNorm: 3-26                             [1, 3, 512]               [1, 3, 512]               1,024\n",
       "│    │    └─MultiHeadAttention: 3-27                    [1, 3, 512]               [1, 3, 512]               1,050,624\n",
       "│    │    └─LayerNorm: 3-28                             [1, 3, 512]               [1, 3, 512]               1,024\n",
       "│    │    └─FeedForward: 3-29                           [1, 3, 512]               [1, 3, 512]               2,099,712\n",
       "│    │    └─LayerNorm: 3-30                             [1, 3, 512]               [1, 3, 512]               1,024\n",
       "│    └─Decoder: 2-8                                     [1, 3, 512]               [1, 3, 512]               --\n",
       "│    │    └─MultiHeadAttention: 3-31                    [1, 3, 512]               [1, 3, 512]               1,050,624\n",
       "│    │    └─LayerNorm: 3-32                             [1, 3, 512]               [1, 3, 512]               1,024\n",
       "│    │    └─MultiHeadAttention: 3-33                    [1, 3, 512]               [1, 3, 512]               1,050,624\n",
       "│    │    └─LayerNorm: 3-34                             [1, 3, 512]               [1, 3, 512]               1,024\n",
       "│    │    └─FeedForward: 3-35                           [1, 3, 512]               [1, 3, 512]               2,099,712\n",
       "│    │    └─LayerNorm: 3-36                             [1, 3, 512]               [1, 3, 512]               1,024\n",
       "│    └─Decoder: 2-9                                     [1, 3, 512]               [1, 3, 512]               --\n",
       "│    │    └─MultiHeadAttention: 3-37                    [1, 3, 512]               [1, 3, 512]               1,050,624\n",
       "│    │    └─LayerNorm: 3-38                             [1, 3, 512]               [1, 3, 512]               1,024\n",
       "│    │    └─MultiHeadAttention: 3-39                    [1, 3, 512]               [1, 3, 512]               1,050,624\n",
       "│    │    └─LayerNorm: 3-40                             [1, 3, 512]               [1, 3, 512]               1,024\n",
       "│    │    └─FeedForward: 3-41                           [1, 3, 512]               [1, 3, 512]               2,099,712\n",
       "│    │    └─LayerNorm: 3-42                             [1, 3, 512]               [1, 3, 512]               1,024\n",
       "│    └─Decoder: 2-10                                    [1, 3, 512]               [1, 3, 512]               --\n",
       "│    │    └─MultiHeadAttention: 3-43                    [1, 3, 512]               [1, 3, 512]               1,050,624\n",
       "│    │    └─LayerNorm: 3-44                             [1, 3, 512]               [1, 3, 512]               1,024\n",
       "│    │    └─MultiHeadAttention: 3-45                    [1, 3, 512]               [1, 3, 512]               1,050,624\n",
       "│    │    └─LayerNorm: 3-46                             [1, 3, 512]               [1, 3, 512]               1,024\n",
       "│    │    └─FeedForward: 3-47                           [1, 3, 512]               [1, 3, 512]               2,099,712\n",
       "│    │    └─LayerNorm: 3-48                             [1, 3, 512]               [1, 3, 512]               1,024\n",
       "│    └─Decoder: 2-11                                    [1, 3, 512]               [1, 3, 512]               --\n",
       "│    │    └─MultiHeadAttention: 3-49                    [1, 3, 512]               [1, 3, 512]               1,050,624\n",
       "│    │    └─LayerNorm: 3-50                             [1, 3, 512]               [1, 3, 512]               1,024\n",
       "│    │    └─MultiHeadAttention: 3-51                    [1, 3, 512]               [1, 3, 512]               1,050,624\n",
       "│    │    └─LayerNorm: 3-52                             [1, 3, 512]               [1, 3, 512]               1,024\n",
       "│    │    └─FeedForward: 3-53                           [1, 3, 512]               [1, 3, 512]               2,099,712\n",
       "│    │    └─LayerNorm: 3-54                             [1, 3, 512]               [1, 3, 512]               1,024\n",
       "│    └─Decoder: 2-12                                    [1, 3, 512]               [1, 3, 512]               --\n",
       "│    │    └─MultiHeadAttention: 3-55                    [1, 3, 512]               [1, 3, 512]               1,050,624\n",
       "│    │    └─LayerNorm: 3-56                             [1, 3, 512]               [1, 3, 512]               1,024\n",
       "│    │    └─MultiHeadAttention: 3-57                    [1, 3, 512]               [1, 3, 512]               1,050,624\n",
       "│    │    └─LayerNorm: 3-58                             [1, 3, 512]               [1, 3, 512]               1,024\n",
       "│    │    └─FeedForward: 3-59                           [1, 3, 512]               [1, 3, 512]               2,099,712\n",
       "│    │    └─LayerNorm: 3-60                             [1, 3, 512]               [1, 3, 512]               1,024\n",
       "├─Linear: 1-3                                           [1, 3, 512]               [1, 3, 10]                5,130\n",
       "==================================================================================================================================\n",
       "Total params: 44,143,626\n",
       "Trainable params: 44,143,626\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 44.14\n",
       "==================================================================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 1.99\n",
       "Params size (MB): 176.57\n",
       "Estimated Total Size (MB): 178.58\n",
       "=================================================================================================================================="
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "# encoder and decoder inputs: (batch, seq_len, d_model)\n",
    "summary(model, input_size=[(1, 3, 512), (1, 3, 512)], col_names=(\"input_size\",\"output_size\",\"num_params\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824a4fe7",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab040cc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "4ffbf193",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, iterator, loss_fn, optimizer, scheduler, d_model, dropout, device):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for batch in iterator:\n",
    "        src_input_ids, tgt_input_ids = batch\n",
    "    \n",
    "        src_input_ids = src_input_ids.to(device)\n",
    "        tgt_input_ids = tgt_input_ids.to(device)\n",
    "        # src_input_ids = torch.cat([tokenize(s, vocab) for s in src_sentences], dim=0).to(device)  # (batch, seq_len)\n",
    "        # tgt_input_ids = torch.cat([tokenize(t, vocab) for t in tgt_sentences], dim=0).to(device)\n",
    "        # src_input_ids = tokenize(src_sentences, vocab).to(device)\n",
    "        # tgt_input_ids = tokenize(tgt_sentences, vocab).to(device)\n",
    "        \n",
    "        tgt_labels = tgt_input_ids.clone()\n",
    "        \n",
    "        src_input_embeddings = embedding(src_input_ids) * math.sqrt(d_model)\n",
    "        src_positional_encoding = positionalEncoding2(src_input_embeddings, d_model)\n",
    "        x_encoder = src_input_embeddings + src_positional_encoding\n",
    "        x_encoder = F.dropout(x_encoder, p=dropout, training=model.training)\n",
    "        \n",
    "        tgt_input_ids = shift_right(tgt_input_ids)\n",
    "        tgt_input_embeddings = embedding(tgt_input_ids) * math.sqrt(d_model)\n",
    "        tgt_positional_encoding = positionalEncoding2(tgt_input_embeddings, d_model)\n",
    "        x_decoder = tgt_input_embeddings + tgt_positional_encoding\n",
    "        x_decoder = F.dropout(x_decoder, p=dropout, training=model.training)\n",
    "        \n",
    "        src_mask = make_padding_mask(src_input_ids, pad_idx=vocab[\"<pad>\"])\n",
    "        tgt_pad_mask = make_padding_mask(tgt_input_ids, pad_idx=vocab[\"<pad>\"])\n",
    "        causal = make_causal_mask(tgt_input_ids.shape[1], device=device)\n",
    "        tgt_mask = combine_padding_and_causal(tgt_pad_mask, causal)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x_encoder, x_decoder, tgt_mask=tgt_mask, src_mask=src_mask)\n",
    "        \n",
    "        loss = loss_fn(logits.view(-1, logits.size(-1)), tgt_labels.view(-1))\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "    return epoch_loss / num_batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "e541b9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# source_sentences = [\"We are friends\",\"I love you\",\"Hello how are you\",\"What is your name\",\"The cat is on the mat\",\"I am a student\",\"She is my sister\",\"The book is on the table\",\"Good morning\",\"Thank you very much\",\"I am from India\",\"How old are you\",\"Where do you live\",\"This is my house\",\"The weather is nice today\",\"I want to eat food\",\"She is reading a book\",\"They are playing cricket\",\"We go to school\",\"He is a doctor\",\"My father is tall\",\"The dog is barking\",\"Please help me\",\"I am happy\",\"See you later\"]\n",
    "# target_sentences = [\"हम दोस्त हैं <eos>\",\"मैं तुमसे प्यार करता हूं <eos>\",\"नमस्ते आप कैसे हैं <eos>\",\"आपका नाम क्या है <eos>\",\"बिल्ली चटाई पर है <eos>\",\"मैं एक छात्र हूं <eos>\",\"वह मेरी बहन है <eos>\",\"किताब मेज पर है <eos>\",\"शुभ प्रभात <eos>\",\"धन्यवाद बहुत बहुत <eos>\",\"मैं भारत से हूं <eos>\",\"आप कितने साल के हैं <eos>\",\"आप कहां रहते हैं <eos>\",\"यह मेरा घर है <eos>\",\"आज मौसम अच्छा है <eos>\",\"मैं खाना खाना चाहता हूं <eos>\",\"वह किताब पढ़ रही है <eos>\",\"वे क्रिकेट खेल रहे हैं <eos>\",\"हम स्कूल जाते हैं <eos>\",\"वह डॉक्टर है <eos>\",\"मेरे पिता लंबे हैं <eos>\",\"कुत्ता भौंक रहा है <eos>\",\"कृपया मेरी मदद करें <eos>\",\"मैं खुश हूं <eos>\",\"बाद में मिलते हैं <eos>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "ea8a81ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"cfilt/iitb-english-hindi\")          # or: load_dataset(\"cfilt/iitb-english-hindi\", \"default\")\n",
    "# Take first 2000 clean pairs\n",
    "small_data = dataset[\"train\"].select(range(2000))\n",
    "source_sentences = [ex[\"translation\"][\"en\"] for ex in small_data]\n",
    "target_sentences = [ex[\"translation\"][\"hi\"] + \" <eos>\" for ex in small_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "2a0e51e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 1542 | Embedding: torch.Size([1542, 512])\n"
     ]
    }
   ],
   "source": [
    "sentences = source_sentences + target_sentences\n",
    "vocab = build_vocab(sentences)\n",
    "vocab_size = len(vocab)\n",
    "id2word = {idx: word for word, idx in vocab.items()}\n",
    "\n",
    "embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=d_model)\n",
    "print(f\"Vocab size: {vocab_size} | Embedding: {embedding.weight.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "ec0b77bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentencePairDataset(Dataset):\n",
    "    def __init__(self, src_sentences, tgt_sentences):\n",
    "        assert len(src_sentences) == len(tgt_sentences)\n",
    "        self.pairs = list(zip(src_sentences, tgt_sentences))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.pairs[idx]\n",
    "\n",
    "def sentence_collate(batch):\n",
    "    # return tuple(zip(*batch))    # (list(src), list(tgt))\n",
    "    src_list, tgt_list = zip(*batch)\n",
    "    \n",
    "    # Pad src\n",
    "    src_lens = [len(tokenize(s, vocab)[0]) for s in src_list]  # lengths\n",
    "    max_src = max(src_lens)\n",
    "    src_padded = torch.full((len(batch), max_src), vocab['<pad>'], dtype=torch.long)\n",
    "    for i, s in enumerate(src_list):\n",
    "        ids = tokenize(s, vocab)[0]  # (seq_len,)\n",
    "        src_padded[i, :len(ids)] = ids\n",
    "    \n",
    "    # Pad tgt (with <eos> already in strings)\n",
    "    tgt_lens = [len(tokenize(t, vocab)[0]) for t in tgt_list]\n",
    "    max_tgt = max(tgt_lens)\n",
    "    tgt_padded = torch.full((len(batch), max_tgt), vocab['<pad>'], dtype=torch.long)\n",
    "    for i, t in enumerate(tgt_list):\n",
    "        ids = tokenize(t, vocab)[0]\n",
    "        tgt_padded[i, :len(ids)] = ids\n",
    "    \n",
    "    return src_padded, tgt_padded  # now returns tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "f017f803",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoamScheduler:\n",
    "    def __init__(self, optimizer, d_model, warmup_steps=4000):\n",
    "        self.optimizer = optimizer\n",
    "        self.d_model = d_model\n",
    "        self.warmup = warmup_steps\n",
    "        self._step = 0\n",
    "\n",
    "    def step(self):\n",
    "        self._step += 1\n",
    "        lr = (self.d_model ** -0.5) * min(self._step ** -0.5,\n",
    "                                         self._step * (self.warmup ** -1.5))\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p['lr'] = lr\n",
    "        return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "deab46ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/_compile.py:54: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information\n",
      "  return disable_fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Train Loss: 6.419 | Train PPL: 613.659\n",
      "Epoch: 02 | Train Loss: 5.216 | Train PPL: 184.108\n",
      "Epoch: 03 | Train Loss: 4.092 | Train PPL:  59.868\n",
      "Epoch: 04 | Train Loss: 3.363 | Train PPL:  28.887\n",
      "Epoch: 05 | Train Loss: 2.963 | Train PPL:  19.363\n",
      "Epoch: 06 | Train Loss: 2.726 | Train PPL:  15.265\n",
      "Epoch: 07 | Train Loss: 2.588 | Train PPL:  13.303\n",
      "Epoch: 08 | Train Loss: 2.452 | Train PPL:  11.609\n",
      "Epoch: 09 | Train Loss: 2.403 | Train PPL:  11.056\n",
      "Epoch: 10 | Train Loss: 2.376 | Train PPL:  10.763\n",
      "Epoch: 11 | Train Loss: 2.324 | Train PPL:  10.219\n",
      "Epoch: 12 | Train Loss: 2.264 | Train PPL:   9.626\n",
      "Epoch: 13 | Train Loss: 2.356 | Train PPL:  10.553\n",
      "Epoch: 14 | Train Loss: 2.287 | Train PPL:   9.850\n",
      "Epoch: 15 | Train Loss: 2.268 | Train PPL:   9.660\n",
      "Epoch: 16 | Train Loss: 2.235 | Train PPL:   9.350\n",
      "Epoch: 17 | Train Loss: 2.221 | Train PPL:   9.216\n",
      "Epoch: 18 | Train Loss: 2.296 | Train PPL:   9.939\n",
      "Epoch: 19 | Train Loss: 2.354 | Train PPL:  10.524\n",
      "Epoch: 20 | Train Loss: 2.581 | Train PPL:  13.213\n",
      "Epoch: 21 | Train Loss: 2.588 | Train PPL:  13.306\n",
      "Epoch: 22 | Train Loss: 2.677 | Train PPL:  14.546\n",
      "Epoch: 23 | Train Loss: 2.688 | Train PPL:  14.703\n",
      "Epoch: 24 | Train Loss: 3.024 | Train PPL:  20.565\n",
      "Epoch: 25 | Train Loss: 3.082 | Train PPL:  21.808\n",
      "Epoch: 26 | Train Loss: 3.083 | Train PPL:  21.826\n",
      "Epoch: 27 | Train Loss: 3.726 | Train PPL:  41.507\n",
      "Epoch: 28 | Train Loss: 3.731 | Train PPL:  41.715\n",
      "Epoch: 29 | Train Loss: 3.676 | Train PPL:  39.471\n",
      "Epoch: 30 | Train Loss: 3.476 | Train PPL:  32.337\n",
      "Epoch: 31 | Train Loss: 3.390 | Train PPL:  29.673\n",
      "Epoch: 32 | Train Loss: 3.448 | Train PPL:  31.433\n",
      "Epoch: 33 | Train Loss: 3.353 | Train PPL:  28.584\n",
      "Epoch: 34 | Train Loss: 3.393 | Train PPL:  29.755\n",
      "Epoch: 35 | Train Loss: 3.333 | Train PPL:  28.030\n",
      "Epoch: 36 | Train Loss: 3.239 | Train PPL:  25.521\n",
      "Epoch: 37 | Train Loss: 3.121 | Train PPL:  22.679\n",
      "Epoch: 38 | Train Loss: 3.150 | Train PPL:  23.339\n",
      "Epoch: 39 | Train Loss: 3.086 | Train PPL:  21.895\n",
      "Epoch: 40 | Train Loss: 3.016 | Train PPL:  20.408\n",
      "Epoch: 41 | Train Loss: 3.011 | Train PPL:  20.307\n",
      "Epoch: 42 | Train Loss: 3.009 | Train PPL:  20.269\n",
      "Epoch: 43 | Train Loss: 2.919 | Train PPL:  18.527\n",
      "Epoch: 44 | Train Loss: 3.048 | Train PPL:  21.065\n",
      "Epoch: 45 | Train Loss: 2.958 | Train PPL:  19.265\n",
      "Epoch: 46 | Train Loss: 2.885 | Train PPL:  17.908\n",
      "Epoch: 47 | Train Loss: 2.839 | Train PPL:  17.095\n",
      "Epoch: 48 | Train Loss: 2.903 | Train PPL:  18.238\n",
      "Epoch: 49 | Train Loss: 2.911 | Train PPL:  18.370\n",
      "Epoch: 50 | Train Loss: 2.864 | Train PPL:  17.530\n",
      "Epoch: 51 | Train Loss: 2.725 | Train PPL:  15.255\n",
      "Epoch: 52 | Train Loss: 2.776 | Train PPL:  16.062\n",
      "Epoch: 53 | Train Loss: 2.650 | Train PPL:  14.160\n",
      "Epoch: 54 | Train Loss: 2.666 | Train PPL:  14.385\n",
      "Epoch: 55 | Train Loss: 2.650 | Train PPL:  14.158\n",
      "Epoch: 56 | Train Loss: 2.597 | Train PPL:  13.430\n",
      "Epoch: 57 | Train Loss: 2.591 | Train PPL:  13.344\n",
      "Epoch: 58 | Train Loss: 2.609 | Train PPL:  13.589\n",
      "Epoch: 59 | Train Loss: 2.548 | Train PPL:  12.777\n",
      "Epoch: 60 | Train Loss: 2.551 | Train PPL:  12.819\n",
      "Epoch: 61 | Train Loss: 2.537 | Train PPL:  12.641\n",
      "Epoch: 62 | Train Loss: 2.516 | Train PPL:  12.383\n",
      "Epoch: 63 | Train Loss: 2.514 | Train PPL:  12.358\n",
      "Epoch: 64 | Train Loss: 2.486 | Train PPL:  12.008\n",
      "Epoch: 65 | Train Loss: 2.496 | Train PPL:  12.139\n",
      "Epoch: 66 | Train Loss: 2.493 | Train PPL:  12.094\n",
      "Epoch: 67 | Train Loss: 2.492 | Train PPL:  12.082\n",
      "Epoch: 68 | Train Loss: 2.432 | Train PPL:  11.379\n",
      "Epoch: 69 | Train Loss: 2.520 | Train PPL:  12.429\n",
      "Epoch: 70 | Train Loss: 2.502 | Train PPL:  12.212\n",
      "Epoch: 71 | Train Loss: 2.517 | Train PPL:  12.388\n",
      "Epoch: 72 | Train Loss: 2.437 | Train PPL:  11.440\n",
      "Epoch: 73 | Train Loss: 2.404 | Train PPL:  11.063\n",
      "Epoch: 74 | Train Loss: 2.400 | Train PPL:  11.027\n",
      "Epoch: 75 | Train Loss: 2.380 | Train PPL:  10.801\n",
      "Epoch: 76 | Train Loss: 2.354 | Train PPL:  10.530\n",
      "Epoch: 77 | Train Loss: 2.357 | Train PPL:  10.558\n",
      "Epoch: 78 | Train Loss: 2.370 | Train PPL:  10.693\n",
      "Epoch: 79 | Train Loss: 2.364 | Train PPL:  10.631\n",
      "Epoch: 80 | Train Loss: 2.366 | Train PPL:  10.657\n",
      "Epoch: 81 | Train Loss: 2.388 | Train PPL:  10.894\n",
      "Epoch: 82 | Train Loss: 2.349 | Train PPL:  10.473\n",
      "Epoch: 83 | Train Loss: 2.338 | Train PPL:  10.363\n",
      "Epoch: 84 | Train Loss: 2.310 | Train PPL:  10.072\n",
      "Epoch: 85 | Train Loss: 2.301 | Train PPL:   9.985\n",
      "Epoch: 86 | Train Loss: 2.295 | Train PPL:   9.920\n",
      "Epoch: 87 | Train Loss: 2.294 | Train PPL:   9.914\n",
      "Epoch: 88 | Train Loss: 2.297 | Train PPL:   9.942\n",
      "Epoch: 89 | Train Loss: 2.279 | Train PPL:   9.763\n",
      "Epoch: 90 | Train Loss: 2.255 | Train PPL:   9.531\n",
      "Epoch: 91 | Train Loss: 2.256 | Train PPL:   9.547\n",
      "Epoch: 92 | Train Loss: 2.270 | Train PPL:   9.682\n",
      "Epoch: 93 | Train Loss: 2.260 | Train PPL:   9.587\n",
      "Epoch: 94 | Train Loss: 2.226 | Train PPL:   9.263\n",
      "Epoch: 95 | Train Loss: 2.200 | Train PPL:   9.023\n",
      "Epoch: 96 | Train Loss: 2.227 | Train PPL:   9.271\n",
      "Epoch: 97 | Train Loss: 2.243 | Train PPL:   9.423\n",
      "Epoch: 98 | Train Loss: 2.213 | Train PPL:   9.143\n",
      "Epoch: 99 | Train Loss: 2.229 | Train PPL:   9.292\n",
      "Epoch: 100 | Train Loss: 2.169 | Train PPL:   8.748\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "dataset = SentencePairDataset(source_sentences, target_sentences)\n",
    "iterator = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    collate_fn=sentence_collate\n",
    ")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=vocab['<pad>'], label_smoothing=0.1)\n",
    "dropout = 0.1\n",
    "d_model = 512\n",
    "heads = 8\n",
    "num_layers = 6\n",
    "model = Transformer(d_model, heads, dropout, vocab_size=len(vocab), num_layers=num_layers, embedding=embedding)\n",
    "model = model.to(device)\n",
    "\n",
    "embedding = embedding.to(device)\n",
    "for p in model.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "if embedding.weight.dim() > 1:\n",
    "    nn.init.xavier_uniform_(embedding.weight)\n",
    "    \n",
    "optimizer = torch.optim.Adam(\n",
    "    list(model.parameters()) + list(embedding.parameters()),\n",
    "    lr=0.0, betas=(0.9, 0.98), eps=1e-9\n",
    ")\n",
    "scheduler = NoamScheduler(optimizer, d_model=d_model, warmup_steps=2000)\n",
    "\n",
    "N_EPOCHS = 100\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss = train_one_epoch(\n",
    "        model, iterator, \n",
    "        loss_fn=loss_fn, optimizer=optimizer, scheduler=scheduler,\n",
    "        d_model=d_model, dropout=dropout, \n",
    "        device=device)\n",
    "    print(f'Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "40be7761",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"pt2.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8adefd",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "5d4b8b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source      : We are friends\n",
      "Translation : पिछली पिछली पिछली पिछली पिछली पिछली पिछली पिछली पिछली पिछली पिछली पिछली पिछली पिछली पिछली पिछली पिछली पिछली पिछली पिछली पिछली पिछली पिछली पिछली पिछली पिछली पिछली पिछली पिछली पिछली\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model.eval()\n",
    "\n",
    "def translate(src_sentence, model, embedding, vocab, positionalEncoding, \n",
    "              make_padding_mask, make_causal_mask, combine_padding_and_causal, \n",
    "              device, max_len=50):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # 1. Encode source (once)\n",
    "        src_ids = tokenize(src_sentence, vocab).to(device)\n",
    "        src_emb = embedding(src_ids) * math.sqrt(d_model)\n",
    "        src_pe = positionalEncoding(src_emb, d_model).to(device)\n",
    "        x_encoder = src_emb + src_pe\n",
    "        src_mask = make_padding_mask(src_ids, pad_idx=vocab[\"<pad>\"])\n",
    "\n",
    "        # 2. Start decoding with <sos>\n",
    "        tgt_ids = torch.tensor([[vocab['<sos>']]], device=device)\n",
    "\n",
    "        for step in range(max_len):\n",
    "            # Decoder input\n",
    "            tgt_emb = embedding(tgt_ids) * math.sqrt(d_model)\n",
    "            tgt_pe = positionalEncoding(tgt_emb, d_model).to(device)\n",
    "            x_decoder = tgt_emb + tgt_pe\n",
    "\n",
    "            # Masks (using your exact functions)\n",
    "            tgt_pad_mask = make_padding_mask(tgt_ids, pad_idx=vocab[\"<pad>\"])\n",
    "            causal = make_causal_mask(tgt_ids.shape[1], device=device)\n",
    "            tgt_mask = combine_padding_and_causal(tgt_pad_mask, causal)\n",
    "\n",
    "            # Forward\n",
    "            logits = model(x_encoder, x_decoder, tgt_mask=tgt_mask, src_mask=src_mask)\n",
    "\n",
    "            # Softmax + argmax (exactly as in the paper diagram)\n",
    "            probs = torch.softmax(logits[0, -1, :], dim=-1)      # (vocab_size,)\n",
    "            next_id = torch.argmax(probs).item()\n",
    "            next_token = torch.tensor([[next_id]], device=device)\n",
    "\n",
    "            # Optional nice debug print\n",
    "            # print(f\"Step {step+1:2d} → {id2word[next_id]:<10} | Conf: {probs[next_id]:.4f}\")\n",
    "\n",
    "            # Append\n",
    "            tgt_ids = torch.cat([tgt_ids, next_token], dim=1)\n",
    "\n",
    "            if next_id == vocab['<eos>']:\n",
    "                break\n",
    "\n",
    "        # Convert to sentence\n",
    "        translated_tokens = [id2word[tok.item()] for tok in tgt_ids[0][1:]]  # skip <sos>\n",
    "        translated = ' '.join(translated_tokens).replace('<eos>', '').strip()\n",
    "\n",
    "        return translated\n",
    "\n",
    "test_sentence = \"We are friends\"\n",
    "print(\"Source      :\", test_sentence)\n",
    "print(\"Translation :\", translate(test_sentence, model, embedding, vocab, \n",
    "                                 positionalEncoding2, make_padding_mask, \n",
    "                                 make_causal_mask, combine_padding_and_causal, \n",
    "                                 device, max_len=30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80650898",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"pt2.pt\", map_location=device)\n",
    "model.load_state_dict(checkpoint)\n",
    "model = model.to(device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
