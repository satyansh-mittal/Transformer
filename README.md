# ğŸ”„ Transformer: Attention Is All You Need

A from-scratch PyTorch implementation of the **["Attention Is All You Need"](https://arxiv.org/abs/1706.03762)** paper for **English â†’ Hindi** machine translation, trained on the [IITB English-Hindi Parallel Corpus](https://huggingface.co/datasets/cfilt/iitb-english-hindi).

---

## ğŸ—ï¸ Architecture

The model follows the original Transformer architecture from the paper:

```
Input (English) â”€â”€â–º Embedding + Positional Encoding â”€â”€â–º Encoder Stack â”€â”€â”
                                                                        â”‚
Input (Hindi)   â”€â”€â–º Embedding + Positional Encoding â”€â”€â–º Decoder Stack â—„â”€â”˜
                                                            â”‚
                                                      Linear + Softmax â”€â”€â–º Output (Hindi)
```

**Key Components:**
- **Scaled Dot-Product Attention** with `Q`, `K`, `V` projections
- **Multi-Head Attention** â€” Parallel attention heads (`d_k = d_model / heads`)
- **Position-wise Feed-Forward Networks** â€” Two linear layers with ReLU
- **Sinusoidal Positional Encoding**
- **Post-Layer Normalization**
- **Weight Tying** â€” Embedding weights shared with the final linear projection
- **Noam LR Scheduler** â€” Warmup-based learning rate schedule from the paper
- **Label Smoothing** â€” Cross-entropy loss with `label_smoothing=0.1`

---

## ğŸ“ Project Structure

```
Transformers/
â”œâ”€â”€ README.md
â”œâ”€â”€ code.ipynb                  # Model 1: Paper-scale Transformer (44M params)
â””â”€â”€ transformers/               # Model 2: Optimized Transformer (15M params)
    â”œâ”€â”€ main.py                 # Entry point â€” train / inference / checkpoint loading
    â”œâ”€â”€ train.py                # Training loop with DDP support
    â”œâ”€â”€ inference.py            # Greedy decoding for translation
    â”œâ”€â”€ transformer.py          # Transformer model (Encoder + Decoder + Linear head)
    â”œâ”€â”€ encoder.py              # Encoder layer (Self-Attention + FFN + LayerNorm)
    â”œâ”€â”€ decoder.py              # Decoder layer (Self-Attn + Cross-Attn + FFN + LayerNorm)
    â”œâ”€â”€ attention.py            # Scaled Dot-Product & Multi-Head Attention
    â”œâ”€â”€ feed_forward.py         # Position-wise Feed-Forward Network
    â”œâ”€â”€ lr_scheduler.py         # Noam LR scheduler
    â”œâ”€â”€ data.py                 # Dataset loading & collation (HuggingFace datasets)
    â”œâ”€â”€ utils.py                # Vocabulary, tokenizer, positional encoding, masks
    â”œâ”€â”€ config.json             # Hyperparameters
    â”œâ”€â”€ model_info.sh           # Script to print model architecture summary
    â”œâ”€â”€ model_summary.txt       # Saved output of model_info.sh
    â”œâ”€â”€ requirements.txt        # Python dependencies
    â”œâ”€â”€ checkpoint.pt           # Final model checkpoint
    â””â”€â”€ checkpoint_best.pt      # Best model checkpoint (lowest loss)
```

---

## ğŸš€ Getting Started

### Prerequisites

- Python 3.11+
- PyTorch 2.x with CUDA support
- NVIDIA GPU(s)

### Installation

```bash
cd transformers/
pip install -r requirements.txt
```

### Training

**Single GPU:**
```bash
python main.py
# Select option 1: Train a new model
```

**Multi-GPU (Distributed Data Parallel):**
```bash
CUDA_VISIBLE_DEVICES=3,4,5,6 torchrun --nproc_per_node=4 main.py
```

### Inference

```bash
python main.py
# Select option 2: Load checkpoint and run inference
# Or option 3: Inference only
```

---

## ğŸ§ª Trained Models

Two Transformer models were trained on the IITB English-Hindi dataset.

---

### Model 1 â€” Paper-Scale Transformer (`code.ipynb`)

This model replicates the exact architecture described in the original paper â€” 6 encoder layers, 6 decoder layers, `d_model=512`, 8 attention heads â€” resulting in **44.14 million parameters**. It was trained on a small subset of 2,000 samples as a proof-of-concept.

| | |
|---|---|
| **Parameters** | 44,143,626 (44.14M) |
| **d_model** | 512 |
| **Heads** | 8 |
| **Encoder / Decoder Layers** | 6 / 6 |
| **FFN Inner Dim** | 2048 |
| **Dataset** | 2,000 samples |
| **GPU** | NVIDIA RTX 3050 Ti (4GB) |
| **Training Time** | ~30 minutes |
| **Train Loss** | 2.169 |
| **Train PPL** | 8.748 |

> The predictions from this model were not good as the dataset was too small (2,000 samples) for a 44M parameter model to learn meaningful translations.

<details>
<summary>ğŸ“‹ Layer-by-layer architecture (click to expand)</summary>

```
Transformer                          [1, 3, 512]    â†’  [1, 3, 10]
â”œâ”€ Encoder Ã— 6
â”‚   â”œâ”€ MultiHeadAttention            [1, 3, 512]    â†’  [1, 3, 512]     1,050,624 params
â”‚   â”œâ”€ LayerNorm                     [1, 3, 512]    â†’  [1, 3, 512]         1,024 params
â”‚   â”œâ”€ FeedForward                   [1, 3, 512]    â†’  [1, 3, 512]     2,099,712 params
â”‚   â””â”€ LayerNorm                     [1, 3, 512]    â†’  [1, 3, 512]         1,024 params
â”œâ”€ Decoder Ã— 6
â”‚   â”œâ”€ MultiHeadAttention (self)     [1, 3, 512]    â†’  [1, 3, 512]     1,050,624 params
â”‚   â”œâ”€ LayerNorm                     [1, 3, 512]    â†’  [1, 3, 512]         1,024 params
â”‚   â”œâ”€ MultiHeadAttention (cross)    [1, 3, 512]    â†’  [1, 3, 512]     1,050,624 params
â”‚   â”œâ”€ LayerNorm                     [1, 3, 512]    â†’  [1, 3, 512]         1,024 params
â”‚   â”œâ”€ FeedForward                   [1, 3, 512]    â†’  [1, 3, 512]     2,099,712 params
â”‚   â””â”€ LayerNorm                     [1, 3, 512]    â†’  [1, 3, 512]         1,024 params
â””â”€ Linear                           [1, 3, 512]    â†’  [1, 3, 10]          5,130 params

Total params: 44,143,626
```

</details>

---

### Model 2 â€” Optimized Transformer (`transformers/`)

This model uses an optimized architecture â€” 4 encoder layers, 4 decoder layers, `d_model=256`, 8 attention heads â€” totaling **15.31 million parameters**. It was trained on the full IITB dataset of 1.66 million sentence pairs using PyTorch Distributed Data Parallel (DDP) across 4 NVIDIA H100 80GB GPUs. This model produces satisfiable English â†’ Hindi translations.

| | |
|---|---|
| **Parameters** | 15,310,480 (15.31M) |
| **d_model** | 256 |
| **Heads** | 8 |
| **Encoder / Decoder Layers** | 4 / 4 |
| **FFN Inner Dim** | 1024 |
| **Vocab Size** | 30,000 (capped) |
| **Max Sequence Length** | 50 |
| **Dataset** | 1,659,083 sentence pairs |
| **Effective Batch Size** | 1,024 (256 Ã— 4 GPUs) |
| **Training Time** | ~10.5 hours (100 epochs) |
| **Train Loss** | 2.786 |
| **Train PPL** | 16.211 |

---

## ğŸ“š Dataset

**[IITB English-Hindi Parallel Corpus](https://huggingface.co/datasets/cfilt/iitb-english-hindi)**

- **Source:** IIT Bombay
- **Training samples:** 1,659,083 sentence pairs
- **Languages:** English â†’ Hindi

---

## ğŸ“ References

- Vaswani, A., et al. *"Attention Is All You Need"*. NeurIPS 2017. [arXiv:1706.03762](https://arxiv.org/abs/1706.03762)
- Anoop Kunchukuttan, Pratik Mehta, and Pushpak Bhattacharyya. *"The IIT Bombay English-Hindi Parallel Corpus"*. LREC 2018.

---

## ğŸ“„ License

This project is for educational and research purposes.
